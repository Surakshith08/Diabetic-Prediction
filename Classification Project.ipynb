# ==============================================================================
# Consolidated Imports from all Jupyter Notebook cells
# ==============================================================================
from matplotlib import pyplot
from pandas import read_csv
from pandas import set_option
from numpy import set_printoptions
from numpy import array
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.ensemble import GradientBoostingClassifier
from pandas.plotting import scatter_matrix
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from pickle import dump
from pickle import load
import numpy # Used implicitly in the notebook for arange and array initialization

# ==============================================================================
# Data Loading and Exploration
# ==============================================================================

# Load CSV using Pandas (Cells 13 & 14)
filename = 'pima-indians-diabetes.csv'
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
data = read_csv(filename, names=names)
print("Data Shape:", data.shape)

# Data Types for Each Attribute (Cell 15)
types = data.dtypes
print("\nData Types:")
print(types)

# Statistical Summary (Cell 16)
print("\nStatistical Summary:")
print(data.describe())
# set_option('precision', 3)
# description = data.describe()
# print(description)

# Pairwise Pearson correlations (Cell 17)
correlations = data.corr(method='pearson')
print("\nPearson Correlations:")
print(correlations)

# Class proportion (Cell 18)
class_counts = data.groupby('class').size()
print("\nClass Counts:")
print(class_counts)

# Univariate Histograms (Cell 19)
print("\nDisplaying Histograms...")
data.hist(figsize = (10, 10))
pyplot.show()

# Density Plots (Cell 20)
print("\nDisplaying Density Plots...")
data.plot(kind='density', subplots=True, layout=(3,3), sharex=False, figsize = (10, 10))
pyplot.show()

# Box and Whisker Plots (Cell 21)
print("\nDisplaying Box Plots...")
data.plot(kind='box', subplots=True, layout=(3,3), sharex=False, sharey=False, figsize = (10, 10))
pyplot.show()

# Correlation Matrix Plot (pyplot) (Cell 22)
print("\nDisplaying Correlation Matrix Plot...")
fig = pyplot.figure(figsize = (10, 10))
ax = fig.add_subplot(111)
cax = ax.matshow(correlations, vmin=-1, vmax=1)
fig.colorbar(cax)
ticks = numpy.arange(0,9,1)
ax.set_xticks(ticks)
ax.set_yticks(ticks)
ax.set_xticklabels(names)
ax.set_yticklabels(names)
pyplot.show()

# Scatterplot Matrix (Cell 23)
print("\nDisplaying Scatterplot Matrix...")
scatter_matrix(data, figsize = (10, 10))
pyplot.show()

# ==============================================================================
# Data Preprocessing
# ==============================================================================

# Prepare Data (Cell 24)
array = data.values
X = array[:,0:8]
Y = array[:,8]

# Preprocess data using Standardization
scaler = StandardScaler().fit(X)
rescaledX = scaler.transform(X)

# summarize transformed data
set_printoptions(precision=3)
# print(rescaledX[0:5,:]) # Uncomment to print rescaled data

# ==============================================================================
# Model Evaluation
# ==============================================================================
num_folds = 10
seed = 7
kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)

# Model Evaluation (Logistic Regression - Raw Data) (Cell 25)
print("\n--- Logistic Regression (Raw Data, KFold) ---")
model = LogisticRegression(max_iter=200)
results = cross_val_score(model, X, Y, cv=kfold)
print("Accuracy: %.3f%% (%.3f%%)" % (results.mean()*100.0, results.std()*100.0))


# Compare Algorithms (Cell 26)
print("\n--- Algorithm Comparison (Raw Data, KFold) ---")
models = []
models.append(('LR', LogisticRegression(max_iter=200)))
models.append(('LDA', LinearDiscriminantAnalysis()))
models.append(('KNN', KNeighborsClassifier()))
models.append(('CART', DecisionTreeClassifier()))
models.append(('NB', GaussianNB()))
models.append(('SVM', SVC()))
models.append(('GBM', GradientBoostingClassifier()))

# evaluate each model in turn
results_comparison = []
names_comparison = []
for name, model in models:
    kfold_comp = KFold(n_splits=10, random_state=seed, shuffle=True)
    cv_results = cross_val_score(model, X, Y, cv=kfold_comp, scoring='accuracy')
    results_comparison.append(cv_results)
    names_comparison.append(name)
    msg = "Accuracy %s: %.3f (%.3f)" % (name, cv_results.mean(), cv_results.std())
    print(msg)

# Boxplot comparison
print("\nDisplaying Algorithm Comparison Boxplot...")
fig = pyplot.figure(figsize = (10, 10))
fig.suptitle('Algorithm Comparison')
ax = fig.add_subplot(111)
pyplot.boxplot(results_comparison, labels=names_comparison)
ax.set_xticklabels(names_comparison)
pyplot.show()


# Improvement on KNN (Standardized Data) (Cell 27)
print("\n--- KNN (Standardized Data, KFold) ---")
kfold_knn = KFold(n_splits=10, random_state=seed, shuffle=True)
model = KNeighborsClassifier()
results = cross_val_score(model, rescaledX, Y, cv=kfold_knn)
print("Accuracy: %.3f%% (%.3f%%)" % (results.mean()*100.0, results.std()*100.0))

# Improvement on SVM (Standardized Data) (Cell 28)
print("\n--- SVM (Standardized Data, KFold) ---")
model = SVC(gamma='auto')
kfold_svm = KFold(n_splits=10, random_state=seed, shuffle=True)
results = cross_val_score(model, rescaledX, Y, cv=kfold_svm)
print("Accuracy: %.3f%% (%.3f%%)" % (results.mean()*100.0, results.std()*100.0))


# ==============================================================================
# Hyperparameter Tuning
# ==============================================================================

# Grid Search for Best K for KNN (Cell 29)
print("\n--- Grid Search for Best K for KNN ---")
model = KNeighborsClassifier()
param_grid = dict(n_neighbors=numpy.array([1,3,5,7,9,11,13,15,17,19,21]))
grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', cv=kfold)
grid_result = grid.fit(rescaledX, Y)

print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("Mean %f (std %f) with: %r" % (mean, stdev, param))

# Grid Search for Best C and Gamma for SVM (Cell 30)
print("\n--- Grid Search for Best C and Gamma for SVM ---")
model = SVC()
param_grid = dict(C=numpy.array([0.1, 0.5, 1.0, 1.5, 2.0]), gamma=numpy.array([0.001, 0.01, 0.1, 0.5, 1.0]))
grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', cv=kfold)
grid_result = grid.fit(rescaledX, Y)

print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("Mean %f (std %f) with: %r" % (mean, stdev, param))

# ==============================================================================
# Finalize Model
# ==============================================================================

# Split the data into train and test sets for final evaluation
array = data.values
X = array[:,0:8]
Y = array[:,8]
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=7)


# Final Logistic Regression Model (Raw Data, Train/Test Split) (Cell 31)
print("\n--- Final Logistic Regression (Raw Data) Evaluation ---")
model = LogisticRegression(max_iter=200)
model.fit(X_train, Y_train)
predicted = model.predict(X_test)

print("Confusion Matrix:")
print(confusion_matrix(Y_test, predicted))
print("\nClassification Report:")
print(classification_report(Y_test, predicted))

# Final SVM Model (Standardized Data, Train/Test Split with Tuned Hyperparameters) (Cell 32)
print("\n--- Final SVM (Standardized Data, Tuned) Evaluation ---")
# Create Scaler and Apply to X_train and X_test
scaler = StandardScaler().fit(X_train)
rescaledX_train = scaler.transform(X_train)
rescaledX_test = scaler.transform(X_test)

# Prepare the SVM model with tuned hyperparameters
model = SVC(C=2.0, gamma=0.01)
model.fit(rescaledX_train, Y_train)
predicted = model.predict(rescaledX_test)

print("Confusion Matrix:")
print(confusion_matrix(Y_test, predicted))
print("\nClassification Report:")
print(classification_report(Y_test, predicted))

# Save Model using Pickle (Cell 33)
print("\n--- Model Saving and Loading Test ---")
# Fit the model on 33%
model = LogisticRegression(max_iter=200)
model.fit(X_train, Y_train)

# save the model to disk
filename_sav = 'finalized_model.sav' # Renamed to avoid collision with filename variable used for CSV
dump(model, open(filename_sav, 'wb'))
print(f"Model saved to '{filename_sav}'")

# some time later...

# load the model from disk
loaded_model = load(open(filename_sav, 'rb'))
result = loaded_model.score(X_test, Y_test)
print("Loaded model accuracy score:", result)


# Data head (Cell 34)
print("\nData Head (First 20 rows):")
print(data.head(20))
